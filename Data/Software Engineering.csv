Control and Data Flow Execution of Java Programs;Safeeullah Soomro, Zainab Alansari, Mohammad Riyaz Belgaum;Since decade understanding of programs has become a compulsory task for the students as well as for others who are involved in the process of developing software and providing solutions to open problems. In that aspect showing the problem in a pictorial presentation in a best manner is a key advantage to better understand it. We provide model and structure for Java programs to understand the control and data flow analysis of execution. Especially it helps to understand the static analysis of Java programs, which is an uttermost important phase for software maintenance. We provided information and model for visualization of Java programs that may help better understanding of programs for a learning and analysis purpose. The idea provided for building visualization tool is extracting data and control analysis from execution of Java programs. We presented case studies to prove that our idea is most important for better understanding of Java programs which may help towards static analysis, software debugging and software maintenance.;https://arxiv.org/abs/1708.07393
Fragmented Monitoring;Oscar Cornejo (University of Milan-Bicocca), Daniela Briola (University of Milan-Bicocca), Daniela Micucci (University of Milan-Bicocca), Leonardo Mariani (University of Milan-Bicocca);Field data is an invaluable source of information for testers and developers because it witnesses how software systems operate in real environments, capturing scenarios and configurations relevant to end-users. Unfortunately, collecting traces might be resource-consuming and can significantly affect the user experience, for instance causing annoying slowdowns.;https://arxiv.org/abs/1708.07232
Exploring the Link Between Test Suite Quality and Automatic Specification Inference;Luke Chircop (University Of Malta), Christian Colombo (University Of Malta), Mark Micallef (University Of Malta);While no one doubts the importance of correct and complete specifications, many industrial systems still do not have formal specifications written out -- and even when they do, it is hard to check their correctness and completeness. This work explores the possibility of using an invariant extraction tool such as Daikon to automatically infer specifications from available test suites with the idea of aiding software engineers to improve the specifications by having another version to compare to. Given that our initial experiments did not produce satisfactory results, in this paper we explore which test suite attributes influence the quality of the inferred specification. Following further study, we found that instruction, branch and method coverage are correlated to high recall values, reaching up to 97.93%.;https://arxiv.org/abs/1708.07231
A Story of Parametric Trace Slicing, Garbage and Static Analysis;Giles Reger (University of Manchester, UK);This paper presents a proposal (story) of how statically detecting unreachable objects (in Java) could be used to improve a particular runtime verification approach (for Java), namely parametric trace slicing. Monitoring algorithms for parametric trace slicing depend on garbage collection to (i) cleanup data-structures storing monitored objects, ensuring they do not become unmanageably large, and (ii) anticipate the violation of (non-safety) properties that cannot be satisfied as a monitored object can no longer appear later in the trace. The proposal is that both usages can be improved by making the unreachability of monitored objects explicit in the parametric property and statically introducing additional instrumentation points generating related events. The ideas presented in this paper are still exploratory and the intention is to integrate the described techniques into the MarQ monitoring tool for quantified event automata.;https://arxiv.org/abs/1708.07228
Trustworthy Refactoring via Decomposition and Schemes: A Complex Case Study;D醤iel Horp醕si, Judit K?szegi, Zolt醤 Horv醫h;Widely used complex code refactoring tools lack a solid reasoning about the correctness of the transformations they implement, whilst interest in proven correct refactoring is ever increasing as only formal verification can provide true confidence in applying tool-automated refactoring to industrial-scale code. By using our strategic rewriting based refactoring specification language, we present the decomposition of a complex transformation into smaller steps that can be expressed as instances of refactoring schemes, then we demonstrate the semi-automatic formal verification of the components based on a theoretical understanding of the semantics of the programming language. The extensible and verifiable refactoring definitions can be executed in our interpreter built on top of a static analyser framework.;https://arxiv.org/abs/1708.07225
Towards Evaluating Size Reduction Techniques for Software Model Checking;Gyula Sallai (Department of Measurement and Information Systems, Budapest University of Technology and Economics), 羕os Hajdu (Department of Measurement and Information Systems, Budapest University of Technology and Economics / MTA-BME Lend黮et Cyber-Physical Systems Research Group), Tam醩 T髏h (Department of Measurement and Information Systems, Budapest University of Technology and Economics), Zolt醤 Micskei (Department of Measurement and Information Systems, Budapest University of Technology and Economics);Formal verification techniques are widely used for detecting design flaws in software systems. Formal verification can be done by transforming an already implemented source code to a formal model and attempting to prove certain properties of the model (e.g. that no erroneous state can occur during execution). Unfortunately, transformations from source code to a formal model often yield large and complex models, making the verification process inefficient and costly. In order to reduce the size of the resulting model, optimization transformations can be used. Such optimizations include common algorithms known from compiler design and different program slicing techniques. Our paper describes a framework for transforming C programs to a formal model, enhanced by various optimizations for size reduction. We evaluate and compare several optimization algorithms regarding their effect on the size of the model and the efficiency of the verification. Results show that different optimizations are more suitable for certain models, justifying the need for a framework that includes several algorithms.;https://arxiv.org/abs/1708.07224
Resilience Design Patterns: A Structured Approach to Resilience at Extreme Scale;Saurabh Hukerikar, Christian Engelmann;Reliability is a serious concern for future extreme-scale high-performance computing (HPC) systems. While the HPC community has developed various resilience solutions, the solution space remains fragmented. There are no formal methods and metrics to integrate the various HPC resilience techniques into composite solutions, nor are there methods to holistically evaluate the adequacy and efficacy of such solutions in terms of their protection coverage, and their performance & power efficiency characteristics. In this paper, we develop a structured approach to the design, evaluation and optimization of HPC resilience using the concept of design patterns. A design pattern is a general repeatable solution to a commonly occurring problem. We identify the problems caused by various types of faults, errors and failures in HPC systems and the techniques used to deal with these events. Each well-known solution that addresses a specific HPC resilience challenge is described in the form of a pattern. We develop a complete catalog of such resilience design patterns, which may be used as essential building blocks when designing and deploying resilience solutions. We also develop a design framework that enhances a designer's understanding the opportunities for integrating multiple patterns across layers of the system stack and the important constraints during implementation of the individual patterns. It is also useful for defining mechanisms and interfaces to coordinate flexible fault management across hardware and software components. The overall goal of this work is to establish a systematic methodology for the design and evaluation of resilience technologies in extreme-scale HPC systems that keep scientific applications running to a correct solution in a timely and cost-efficient manner despite frequent faults, errors, and failures of various types.;https://arxiv.org/abs/1708.07422
Results of the Survey: Failures in Robotics and Intelligent Systems;Johannes Wienke, Sebastian Wrede;In January 2015 we distributed an online survey about failures in robotics and intelligent systems across robotics researchers. The aim of this survey was to find out which types of failures currently exist, what their origins are, and how systems are monitored and debugged - with a special focus on performance bugs. This report summarizes the findings of the survey.;https://arxiv.org/abs/1708.07379
Control-Flow Residual Analysis for Symbolic Automata;Shaun Azzopardi (University of Malta), Christian Colombo (University of Malta), Gordon J. Pace (University of Malta);Where full static analysis of systems fails to scale up due to system size, dynamic monitoring has been increasingly used to ensure system correctness. The downside is, however, runtime overheads which are induced by the additional monitoring code instrumented. To address this issue, various approaches have been proposed in the literature to use static analysis in order to reduce monitoring overhead. In this paper we generalise existing work which uses control-flow static analysis to optimise properties specified as automata, and prove how similar analysis can be applied to more expressive symbolic automata - enabling reduction of monitoring instrumentation in the system, and also monitoring logic. We also present empirical evidence of the effectiveness of this approach through an analysis of the effect of monitoring overheads in a financial transaction system.;https://arxiv.org/abs/1708.07230
A Survey of Runtime Monitoring Instrumentation Techniques;Ian Cassar (University of Malta and Reykjavik University), Adrian Francalanza (University of Malta), Luca Aceto (Reykjavik University), Anna Ing髄fsd髏tir (Reykjavik University);Runtime Monitoring is a lightweight and dynamic verification technique that involves observing the internal operations of a software system and/or its interactions with other external entities, with the aim of determining whether the system satisfies or violates a correctness specification. Compilation techniques employed in Runtime Monitoring tools allow monitors to be automatically derived from high-level correctness specifications (aka. properties). This allows the same property to be converted into different types of monitors, which may apply different instrumentation techniques for checking whether the property was satisfied or not. In this paper we compare and contrast the various types of monitoring methodologies found in the current literature, and classify them into a spectrum of monitoring instrumentation techniques, ranging from completely asynchronous monitoring on the one end and completely synchronous monitoring on the other.;https://arxiv.org/abs/1708.07229
Paving the Roadway for Safety of Automated Vehicles: An Empirical Study on Testing Challenges;Alessia Knauss, Jan Schr鰀er, Christian Berger, Henrik Eriksson;The technology in the area of automated vehicles is gaining speed and promises many advantages. However, with the recent introduction of conditionally automated driving, we have also seen accidents. Test protocols for both, conditionally automated (e.g., on highways) and automated vehicles do not exist yet and leave researchers and practitioners with different challenges. For instance, current test procedures do not suffice for fully automated vehicles, which are supposed to be completely in charge for the driving task and have no driver as a back up. This paper presents current challenges of testing the functionality and safety of automated vehicles derived from conducting focus groups and interviews with 26 participants from five countries having a background related to testing automotive safety-related topics.We provide an overview of the state-of-practice of testing active safety features as well as challenges that needs to be addressed in the future to ensure safety for automated vehicles. The major challenges identified through the interviews and focus groups, enriched by literature on this topic are related to 1) virtual testing and simulation, 2) safety, reliability, and quality, 3) sensors and sensor models, 4) required scenario complexity and amount of test cases, and 5) handover of responsibility between the driver and the vehicle.;https://arxiv.org/abs/1708.06988
Systematic Innovation Mounted Software Development Process and Intuitive Project Management Framework for Lean Startups;Song-Kyoo Kim;Today, software products are required to be more innovative and attractive because of unique circumstances in the software industry. Markets are changing fast and customers want to have more innovative products immediately. This paper provides a new process which integrates an inventive problem solving method into one modern software development program, making it part of the software development process. The Systematic Innovation Mounted Software Development Process (SPI), a combination of Agile and Systematic Innovation, provides an alternative development process which is targeted to adapt idea generation into software products. The intuitive project management framework helps technology driven companies to manage their software projects more effectively. This new software development process and associated techniques could impact the current software development industry significantly, especially software startup companies, because these powerful tools can help reduce managerial workloads of the companies and give them more time to remain focused on their key technologies.;https://arxiv.org/abs/1708.06900
Proceedings Fifth International Workshop on Verification and Program Transformation;Alexei Lisitsa (University of Liverpool, UK), Andrei P. Nemytykh (ISPRAS, Russia), Maurizio Proietti (CNR-IASI, Italy);"This volume contains the proceedings of the Fifth International Workshop on Verification and Program Transformation (VPT 2017). The workshop took place in Uppsala, Sweden, on April 29th, 2017, affiliated with the European Joint Conferences on Theory and Practice of Software (ETAPS). The aim of the VPT workshop series is to provide a forum where people from the areas of program transformation and program verification can fruitfully exchange ideas and gain a deeper understanding of the interactions between those two fields. Seven papers were presented at the workshop. Additionally, three invited talks were given by Javier Esparza (Technische Universit\""at M\""unchen, Germany), Manuel Hermenegildo (IMDEA Software Institute, Madrid, Spain), and Alexey Khoroshilov (Linux Verification Center, ISPRAS, Moscow, Russia).";https://arxiv.org/abs/1708.06887
Software engineering and the SP theory of intelligence;J Gerard Wolff;"This paper describes a novel approach to software engineering derived from the ""SP theory of intelligence"" and its realisation in the ""SP computer model"". These are the bases of a projected industrial-strength ""SP machine"" which, when mature, is anticipated to be the vehicle for software engineering as described in this paper. Potential benefits of this new approach to software engineering include: the automation of semi-automation of software development, with non-automatic programming of the SP system where necessary allowing programmers to concentrate on 'real-world' parallelism, without worries about parallelism to speed up processing the ambitious long-term goal of programming the SP system via written or spoken natural language reducing or eliminating the distinction between 'design' and 'implementation' reducing or eliminating operations like compiling or interpretation reducing or eliminating the need for verification of software reducing the need for an explicit process of validation of software no formal distinction between program and database potential for substantial reductions in the number of types of data file and the number of computer languages benefits for version control and reducing technical debt.";https://arxiv.org/abs/1708.06665
Finding Regressions in Projects under Version Control Systems;Jaroslav Bendik, Nikola Benes, Ivana Cerna;Version Control Systems (VCS) are frequently used to support development of large-scale software projects. A typical VCS repository of a large project can contain various intertwined branches consisting of a large number of commits. If some kind of unwanted behaviour (e.g. a bug in the code) is found in the project, it is desirable to find the commit that introduced it. Such commit is called a regression point. There are two main issues regarding the regression points. First, detecting whether the project after a certain commit is correct can be very expensive as it may include large-scale testing and/or some other forms of verification. It is thus desirable to minimise the number of such queries. Second, there can be several regression points preceding the actual commit perhaps a bug was introduced in a certain commit, inadvertently fixed several commits later, and then reintroduced in a yet later commit. In order to fix the actual commit it is usually desirable to find the latest regression point.;https://arxiv.org/abs/1708.06623
Towards Object Life Cycle-Based Variant Generation of Business Process Models;Ahmed Tealeb;Variability management of process models is a major challenge for Process-Aware Information Systems. Process model variants can be attributed to any of the following reasons: new technologies, governmental rules, organizational context or adoption of new standards. Current approaches to manage variants of process models address issues such as reducing the huge effort of modeling from scratch, preventing redundancy, and controlling inconsistency in process models. Although the effort to manage process model variants has been exerted, there are still limitations. Furthermore, existing approaches do not focus on variants that come from change in organizational perspective of process models. Organizational-driven variant management is an important area that still needs more study that we focus on in this paper. Object Life Cycle (OLC) is an important aspect that may change from an organization to another. This paper introduces an approach inspired by real life scenario to generate consistent process model variants that come from adaptations in the OLC.;https://arxiv.org/abs/1708.06596
nuts-flow/ml: data pre-processing for deep learning;S. Maetschke, R. Tennakoon, C. Vecchiola, R. Garnavi;Data preprocessing is a fundamental part of any machine learning application and frequently the most time-consuming aspect when developing a machine learning solution. Preprocessing for deep learning is characterized by pipelines that lazily load data and perform data transformation, augmentation, batching and logging. Many of these functions are common across applications but require different arrangements for training, testing or inference. Here we introduce a novel software framework named nuts-flow/ml that encapsulates common preprocessing operations as components, which can be flexibly arranged to rapidly construct efficient preprocessing pipelines for deep learning.;https://arxiv.org/abs/1708.06046
A novel agent-based simulation framework for sensing in complex adaptive environments;Muaz A. Niazi, Amir Hussain;In this paper we present a novel Formal Agent-Based Simulation framework (FABS). FABS uses formal specification as a means of clear description of wireless sensor networks (WSN) sensing a Complex Adaptive Environment. This specification model is then used to develop an agent-based model of both the wireless sensor network as well as the environment. As proof of concept, we demonstrate the application of FABS to a boids model of self-organized flocking of animals monitored by a random deployment of proximity sensors.;https://arxiv.org/abs/1708.05875
Learning Effective Changes For Software Projects;Rahul Krishna, Tim Menzies;"The current generation of software analytics tools are mostly prediction algorithms (e.g. support vector machines, naive bayes, logistic regression, etc). While prediction is useful, after prediction comes planning about what actions to take in order to improve quality. This research seeks methods that support actionable analytics that offer clear guidance on ""what to do"" within the context of a specific software project. Specifically, we propose the BELLTREE algorithm for generating plans to improve software quality. Each such plan has the property that, if followed, it reduces the probability of future defect reports. When compared to other planning algorithms from the SE literature, we find that BELLTREE is most effective at learning plans from one project, then applying those plans to another.";https://arxiv.org/abs/1708.05442
