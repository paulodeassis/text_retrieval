Power Optimizations in MTJ-based Neural Networks through Stochastic Computing;Ankit Mondal, Ankur Srivastava;Artificial Neural Networks (ANNs) have found widespread applications in tasks such as pattern recognition and image classification. However, hardware implementations of ANNs using conventional binary arithmetic units are computationally expensive, energy-intensive and have large area overheads. Stochastic Computing (SC) is an emerging paradigm which replaces these conventional units with simple logic circuits and is particularly suitable for fault-tolerant applications. Spintronic devices, such as Magnetic Tunnel Junctions (MTJs), are capable of replacing CMOS in memory and logic circuits. In this work, we propose an energy-efficient use of MTJs, which exhibit probabilistic switching behavior, as Stochastic Number Generators (SNGs), which forms the basis of our NN implementation in the SC domain. Further, error resilient target applications of NNs allow us to introduce Approximate Computing, a framework wherein accuracy of computations is traded-off for substantial reductions in power consumption. We propose approximating the synaptic weights in our MTJ-based NN implementation, in ways brought about by properties of our MTJ-SNG, to achieve energy-efficiency. We design an algorithm that can perform such approximations within a given error tolerance in a single-layer NN in an optimal way owing to the convexity of the problem formulation. We then use this algorithm and develop a heuristic approach for approximating multi-layer NNs. To give a perspective of the effectiveness of our approach, a 43% reduction in power consumption was obtained with less than 1% accuracy loss on a standard classification problem, with 26% being brought about by the proposed algorithm.;https://arxiv.org/abs/1709.04322;;;
An efficient genetic algorithm for large-scale planning of robust industrial wireless networks;Xu Gong, David Plets, Emmeric Tanghe, Toon De Pessemier, Luc Martens, Wout Joseph;An industrial indoor environment is harsh for wireless communications compared to an office environment, because the prevalent metal easily causes shadowing effects and affects the availability of an industrial wireless local area network (IWLAN). On the one hand, it is costly, time-consuming, and ineffective to perform trial-and-error manual deployment of wireless nodes. On the other hand, the existing wireless planning tools only focus on office environments such that it is hard to plan IWLANs due to the larger problem size and the deployed IWLANs are vulnerable to prevalent shadowing effects in harsh industrial indoor environments. To fill this gap, this paper proposes an overdimensioning model and a genetic algorithm based over-dimensioning (GAOD) algorithm for deploying large-scale robust IWLANs. As a progress beyond the state-of-the-art wireless planning, two full coverage layers are created. The second coverage layer serves as redundancy in case of shadowing. Meanwhile, the deployment cost is reduced by minimizing the number of access points (APs)  the hard constraint of minimal inter-AP spatial paration avoids multiple APs covering the same area to be simultaneously shadowed by the same obstacle. The computation time and occupied memory are dedicatedly considered in the design of GAOD for large-scale optimization. A greedy heuristic based over-dimensioning (GHOD) algorithm and a random OD algorithm are taken as benchmarks. In two vehicle manufacturers with a small and large indoor environment, GAOD outperformed GHOD with up to 20% less APs, while GHOD outputted up to 25% less APs than a random OD algorithm. Furthermore, the effectiveness of this model and GAOD was experimentally validated with a real deployment system.;https://arxiv.org/abs/1709.04321;https://arxiv.org/abs/1709.04321;;
An efficient genetic algorithm for large-scale transmit power control of dense industrial wireless networks;Xu Gong, David Plets, Emmeric Tanghe, Toon De Pessemier, Luc Martens, Wout Joseph;The industrial wireless local area network (IWLAN) is increasingly dense, not only due to the penetration of wireless applications into factories and warehouses, but also because of the rising need of redundancy for robust wireless coverage. Instead of powering on all the nodes with the maximal transmit power, it becomes an unavoidable challenge to control the transmit power of all wireless nodes on a large scale, in order to reduce interference and adapt coverage to the latest shadowing effects in the environment. Therefore, this paper proposes an efficient genetic algorithm (GA) to solve this transmit power control (TPC) problem for dense IWLANs, named GATPC. Effective population initialization, crossover and mutation, parallel computing as well as dedicated speedup measures are introduced to tailor GATPC for the large-scale optimization that is intrinsically involved in this problem. In contrast to most coverage-related optimization algorithms which cannot deal with the prevalent shadowing effects in harsh industrial indoor environments, an empirical one-slope path loss model considering three-dimensional obstacle shadowing effects is used in GATPC, in order to enable accurate yet simple coverage prediction. Experimental validation and numerical experiments in real industrial cases show the promising performance of GATPC in terms of scalability to a hyper-large scale, up to 37-times speedup in resolution runtime, and solution quality to achieve adaptive coverage and to minimize interference.;https://arxiv.org/abs/1709.04320;;;
Enhanced Particle Swarm Optimization Algorithms for Multiple-Input Multiple-Output System Modelling using Convolved Gaussian Process Models;Gang Cao, Edmund M-K Lai, Fakhrul Alam;Convolved Gaussian Process (CGP) is able to capture the correlations not only between inputs and outputs but also among the outputs. This allows a superior performance of using CGP than standard Gaussian Process (GP) in the modelling of Multiple-Input Multiple-Output (MIMO) systems when observations are missing for some of outputs. Similar to standard GP, a key issue of CGP is the learning of hyperparameters from a set of input-output observations. It typically performed by maximizing the Log-Likelihood (LL) function which leads to an unconstrained nonlinear and non-convex optimization problem. Algorithms such as Conjugate Gradient (CG) or Broyden-Fletcher-Goldfarb-Shanno (BFGS) are commonly used but they often get stuck in local optima, especially for CGP where there are more hyperparameters. In addition, the LL value is not a reliable indicator for judging the quality intermediate models in the optimization process. In this paper, we propose to use enhanced Particle Swarm Optimization (PSO) algorithms to solve this problem by minimizing the model output error instead. This optimization criterion enables the quality of intermediate solutions to be directly observable during the optimization process. Two enhancements to the standard PSO algorithm which make use of gradient information and the multi- start technique are proposed. Simulation results on the modelling of both linear and nonlinear systems demonstrate the effectiveness of minimizing the model output error to learn hyperparameters and the performance of using enhanced algorithms.;https://arxiv.org/abs/1709.04319;;;
Predictive modeling of die filling of the pharmaceutical granules using the flexible neural tree;Varun Kumar Ojha, Serena Schiano, Chuan-Yu Wu, Václav Snášel, Ajith Abraham;In this work, a computational intelligence (CI) technique named flexible neural tree (FNT) was developed to predict die filling performance of pharmaceutical granules and to identify significant die filling process variables. FNT resembles feedforward neural network, which creates a tree-like structure by using genetic programming. To improve accuracy, FNT parameters were optimized by using differential evolution algorithm. The performance of the FNT-based CI model was evaluated and compared with other CI techniques: multilayer perceptron, Gaussian process regression, and reduced error pruning tree. The accuracy of the CI model was evaluated experimentally using die filling as a case study. The die filling experiments were performed using a model shoe system and three different grades of microcrystalline cellulose (MCC) powders (MCC PH 101, MCC PH 102, and MCC DG). The feed powders were roll-compacted and milled into granules. The granules were then sieved into samples of various size classes. The mass of granules deposited into the die at different shoe speeds was measured. From these experiments, a dataset consisting true density, mean diameter (d50), granule size, and shoe speed as the inputs and the deposited mass as the output was generated. Cross-validation (CV) methods such as 10FCV and 5x2FCV were applied to develop and to validate the predictive models. It was found that the FNT-based CI model (for both CV methods) performed much better than other CI models. Additionally, it was observed that process variables such as the granule size and the shoe speed had a higher impact on the predictability than that of the powder property such as d50. Furthermore, validation of model prediction with experimental data showed that the die filling behavior of coarse granules could be better predicted than that of fine granules.;https://arxiv.org/abs/1709.04318;;;
Pattern Recognition using Artificial Immune System;Mohammad Tarek Al Muallim;In this thesis, the uses of Artificial Immune Systems (AIS) in Machine learning is studded. the thesis focus on some of immune inspired algorithms such as clonal selection algorithm and artificial immune network. The effect of changing the algorithm parameter on its performance is studded. Then a new immune inspired algorithm for unsupervised classification is proposed. The new algorithm is based on clonal selection principle and named Unsupervised Clonal Selection Classification (UCSC). The new proposed algorithm is almost parameter free. The algorithm parameters are data driven and it adjusts itself to make the classification as fast as possible. The performance of UCSC is evaluated. The experiments show that the proposed UCSC algorithm has a good performance and more reliable.;https://arxiv.org/abs/1709.04317;;;
Parallelizing Linear Recurrent Neural Nets Over Sequence Length;Eric Martin, Chris Cundy;Recurrent neural networks (RNNs) are widely used to model sequential data but their non-linear dependencies between sequence elements prevent parallelizing training over sequence length. We show the training of RNNs with only linear sequential dependencies can be parallelized over the sequence length using the parallel scan algorithm, leading to rapid training on long sequences with small minibatch size. We abstract prior linear sequence models into a new framework of linear surrogate RNNs and develop a linear surrogate long short-term memory (LS-LSTM) powered by a parallel linear recurrence CUDA kernel we implemented. We evaluate the LS-LSTM on a long sequence noisy autoregressive task and find the LS-LSTM achieves slightly superior train and test performance to a similar sized LSTM in 4x less training time. We analyze latency and throughput of the LS-LSTM and find the LS-LSTM reaches up to 175x the throughput of the LSTM in the small minibatch long sequence regime.;https://arxiv.org/abs/1709.04057;;;
Shifting Mean Activation Towards Zero with Bipolar Activation Functions;Lars Eidnes, Arild Nøkland;We propose a simple extension to the ReLU-family of activation functions that allows them to shift the mean activation across a layer towards zero. Combined with proper weight initialization, this alleviates the need for normalization layers. We explore the training of deep vanilla recurrent neural networks (RNNs) with up to 144 layers, and show that bipolar activation functions help learning in this setting. On the Penn Treebank and Text8 language modeling tasks we obtain competitive results, improving on the best reported results for non-gated networks.;https://arxiv.org/abs/1709.04054;;;
Spatio-temporal Learning with Arrays of Analog Nanosynapses;Christopher H. Bennett, Damien Querlioz, Jacques-Olivier Klein;Emerging nanodevices such as resistive memories are being considered for hardware realizations of a variety of artificial neural networks (ANNs), including highly promising online variants of the learning approaches known as reservoir computing (RC) and the extreme learning machine (ELM). We propose an RC/ELM inspired learning system built with nanosynapses that performs both on-chip projection and regression operations. To address time-dynamic tasks, the hidden neurons of our system perform spatio-temporal integration and can be further enhanced with variable sampling or multiple activation windows. We detail the system and show its use in conjunction with a highly analog nanosynapse device on a standard task with intrinsic timing dynamics- the TI-46 battery of spoken digits. The system achieves nearly perfect (99%) accuracy at sufficient hidden layer size, which compares favorably with software results. In addition, the model is extended to a larger dataset, the MNIST database of handwritten digits. By translating the database into the time domain and using variable integration windows, up to 95% classification accuracy is achieved. In addition to an intrinsically low-power programming style, the proposed architecture learns very quickly and can easily be converted into a spiking system with negligible loss in performance- all features that confer significant energy efficiency.;https://arxiv.org/abs/1709.03849;;;
Opportunistic Self Organizing Migrating Algorithm for Real-Time Dynamic Traveling Salesman Problem;Shubham Dokania, Sunyam Bagga, Rohit Sharma;Self Organizing Migrating Algorithm (SOMA) is a meta-heuristic algorithm based on the self-organizing behavior of individuals in a simulated social environment. SOMA performs iterative computations on a population of potential solutions in the given search space to obtain an optimal solution. In this paper, an Opportunistic Self Organizing Migrating Algorithm (OSOMA) has been proposed that introduces a novel strategy to generate perturbations effectively. This strategy allows the individual to span across more possible solutions and thus, is able to produce better solutions. A comprehensive analysis of OSOMA on multi-dimensional unconstrained benchmark test functions is performed. OSOMA is then applied to solve real-time Dynamic Traveling Salesman Problem (DTSP). The problem of real-time DTSP has been stipulated and simulated using real-time data from Google Maps with a varying cost-metric between any two cities. Although DTSP is a very common and intuitive model in the real world, its presence in literature is still very limited. OSOMA performs exceptionally well on the problems mentioned above. To substantiate this claim, the performance of OSOMA is compared with SOMA, Differential Evolution and Particle Swarm Optimization.;https://arxiv.org/abs/1709.03793;;;
Multimodal Content Analysis for Effective Advertisements on YouTube;Nikhita Vedula, Wei Sun, Hyunhwan Lee, Harsh Gupta, Mitsunori Ogihara, Joseph Johnson, Gang Ren, Srinivasan Parthasarathy;The rapid advances in e-commerce and Web 2.0 technologies have greatly increased the impact of commercial advertisements on the general public. As a key enabling technology, a multitude of recommender systems exists which analyzes user features and browsing patterns to recommend appealing advertisements to users. In this work, we seek to study the characteristics or attributes that characterize an effective advertisement and recommend a useful set of features to aid the designing and production processes of commercial advertisements. We analyze the temporal patterns from multimedia content of advertisement videos including auditory, visual and textual components, and study their individual roles and synergies in the success of an advertisement. The objective of this work is then to measure the effectiveness of an advertisement, and to recommend a useful set of features to advertisement designers to make it more successful and approachable to users. Our proposed framework employs the signal processing technique of cross modality feature learning where data streams from different components are employed to train separate neural network models and are then fused together to learn a shared representation. Subsequently, a neural network model trained on this joint feature embedding representation is utilized as a classifier to predict advertisement effectiveness. We validate our approach using subjective ratings from a dedicated user study, the sentiment strength of online viewer comments, and a viewer opinion metric of the ratio of the Likes and Views received by each advertisement from an online platform.;https://arxiv.org/abs/1709.03946;;;
Evolution of Convolutional Highway Networks;Oliver Kramer;Convolutional highways are deep networks based on multiple stacked convolutional layers for feature preprocessing. We introduce an evolutionary algorithm (EA) for optimization of the structure and hyperparameters of convolutional highways and demonstrate the potential of this optimization setting on the well-known MNIST data set. The (1+1)-EA employs Rechenberg's mutation rate control and a niching mechanism to overcome local optima adapts the optimization approach. An experimental study shows that the EA is capable of improving the state-of-the-art network contribution and of evolving highway networks from scratch.;https://arxiv.org/abs/1709.03247;;;
Applying ACO To Large Scale TSP Instances;Darren M. Chitty;Ant Colony Optimisation (ACO) is a well known metaheuristic that has proven successful at solving Travelling Salesman Problems (TSP). However, ACO suffers from two issues  the first is that the technique has significant memory requirements for storing pheromone levels on edges between cities and second, the iterative probabilistic nature of choosing which city to visit next at every step is computationally expensive. This restricts ACO from solving larger TSP instances. This paper will present a methodology for deploying ACO on larger TSP instances by removing the high memory requirements, exploiting parallel CPU hardware and introducing a significant efficiency saving measure. The approach results in greater accuracy and speed. This enables the proposed ACO approach to tackle TSP instances of up to 200K cities within reasonable timescales using a single CPU. Speedups of as much as 1200 fold are achieved by the technique.;https://arxiv.org/abs/1709.03187;https://arxiv.org/abs/1709.03187;;
A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data;Abien Fred Agarap;Gated Recurrent Unit (GRU) is a recently published variant of the Long Short-Term Memory (LSTM) network, designed to solve the vanishing gradient and exploding gradient problems. However, its main objective is to solve the long-term dependency problem in Recurrent Neural Networks (RNNs), which prevents the network to connect an information from previous iteration with the current iteration. This study proposes a modification on the GRU model, having Support Vector Machine (SVM) as its classifier instead of the Softmax function. The classifier is responsible for the output of a network in a classification problem. SVM was chosen over Softmax for its computational efficiency. To evaluate the proposed model, it will be used for intrusion detection, with the dataset from Kyoto University's honeypot system in 2013 which will serve as both its training and testing data.;https://arxiv.org/abs/1709.03082;;;
Variable Annealing Length and Parallelism in Simulated Annealing;Vincent A. Cicirello;In this paper, we propose: (a) a restart schedule for an adaptive simulated annealer, and (b) parallel simulated annealing, with an adaptive and parameter-free annealing schedule. The foundation of our approach is the Modified Lam annealing schedule, which adaptively controls the temperature parameter to track a theoretically ideal rate of acceptance of neighboring states. A sequential implementation of Modified Lam simulated annealing is almost parameter-free. However, it requires prior knowledge of the annealing length. We eliminate this parameter using restarts, with an exponentially increasing schedule of annealing lengths. We then extend this restart schedule to parallel implementation, executing several Modified Lam simulated annealers in parallel, with varying initial annealing lengths, and our proposed parallel annealing length schedule. To validate our approach, we conduct experiments on an NP-Hard scheduling problem with sequence-dependent setup constraints. We compare our approach to fixed length restarts, both sequentially and in parallel. Our results show that our approach can achieve substantial performance gains, throughout the course of the run, demonstrating our approach to be an effective anytime algorithm.;https://arxiv.org/abs/1709.02877;;;
NiftyNet: a deep-learning platform for medical imaging;Eli Gibson, Wenqi Li, Carole Sudre, Lucas Fidon, Dzoshkun Shakir, Guotai Wang, Zach Eaton-Rosen, Robert Gray, Tom Doel, Yipeng Hu, Tom Whyntie, Parashkev Nachev, Dean C. Barratt, Sébastien Ourselin, M. Jorge Cardoso, Tom Vercauteren;Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions. This has resulted is substantial duplication of effort and incompatible infrastructure across many research groups. This work presents the open-source NiftyNet platform for deep learning in medical imaging. The ambition of NiftyNet is to accelerate and simplify the development of these solutions, and to provide a common mechanism for disseminating research outputs for the community to use, adapt and build upon. This TensorFlow-based infrastructure provides a complete modular deep learning pipeline for a range of medical imaging applications including segmentation, regression, image generation and representation learning applications with data loading, data augmentation, network architectures, loss functions and evaluation metrics that are tailored to, and take advantage of, the idiosyncracies of medical image analysis and computer-assisted interventions.;https://arxiv.org/abs/1709.03485;;;
UI-Net: Interactive Artificial Neural Networks for Iterative Image Segmentation Based on a User Model;Mario Amrehn, Sven Gaube, Mathias Unberath, Frank Schebesch, Tim Horz, Maddalena Strumia, Stefan Steidl, Markus Kowarschik, Andreas Maier;For complex segmentation tasks, fully automatic systems are inherently limited in their achievable accuracy for extracting relevant objects. Especially in cases where only few data sets need to be processed for a highly accurate result, semi-automatic segmentation techniques exhibit a clear benefit for the user. One area of application is medical image processing during an intervention for a single patient. We propose a learning-based cooperative segmentation approach which includes the computing entity as well as the user into the task. Our system builds upon a state-of-the-art fully convolutional artificial neural network (FCN) as well as an active user model for training. During the segmentation process, a user of the trained system can iteratively add additional hints in form of pictorial scribbles as seed points into the FCN system to achieve an interactive and precise segmentation result. The segmentation quality of interactive FCNs is evaluated. Iterative FCN approaches can yield superior results compared to networks without the user input channel component, due to a consistent improvement in segmentation quality after each interaction.;https://arxiv.org/abs/1709.03450;;;
On the exact relationship between the denoising function and the data distribution;Heikki Arponen, Matti Herranen, Harri Valpola;We prove an exact relationship between the optimal denoising function and the data distribution in the case of additive Gaussian noise, showing that denoising implicitly models the structure of data allowing it to be exploited in the unsupervised learning of representations. This result generalizes a known relationship [2], which is valid only in the limit of small corruption noise.;https://arxiv.org/abs/1709.02797;;;
A Real-time Trainable and Clock-less Spiking Neural Network with 1R Memristive Synapses;Aditya Shukla, Udayan Ganguly;Owing to their unexplored but potentially superior computational capability and remarkably low power consumption for executing brain-like tasks, spiking neural networks (SNNs) have come under the scope of several research groups. For modeling a network of spiking neurons and synapses highly parallelized hardware with a distributed and localized processor and memory is essential. Highly integrable resistive RAM or ReRAM, with voltage-pulse moldable analog conductivity, has a potential to play a key role in such a hardware model, as it can and has been shown to mimic a synapse and its time dependent plasticity. Being a two terminal device, however, it is not directly suitable for naturally/biologically asynchronous SNNs because it requires ability to do reading and writing concurrently. In this work, we overcome this challenge and present a clock-less approach to implement both learning and recognition on a spiking neural network with memristive synaptic array wherein, reading and writing are performed in different frequency domains. Further, we validate our scheme in SPICE by translating a mathematical mono-layered feed-forward Iris classifying SNN. A hardware implementation of the scheme will eliminate the need for clocks and switches to drive learning and has the potential to be used in the development of real-time trainable hardware recurrent networks.;https://arxiv.org/abs/1709.02699;;;
What Weights Work for You? Adapting Weights for Any Pareto Front Shape in Decomposition-based Evolutionary Multi-Objective Optimisation;Miqing Li, Xin Yao;The quality of solution sets generated by decomposition-based evolutionary multiobjective optimisation (EMO) algorithms depends heavily on the consistency between a given problem's Pareto front shape and the specified weights' distribution. A set of weights distributed uniformly in a simplex often lead to a set of well-distributed solutions on a Pareto front with a simplex-like shape, but may fail on other Pareto front shapes. It is an open problem on how to specify a set of appropriate weights without the information of the problem's Pareto front beforehand. In this paper, we propose an approach to adapt the weights during the evolutionary process (called AdaW). AdaW progressively seeks a suitable distribution of weights for the given problem by elaborating five parts in the weight adaptation --- weight generation, weight addition, weight deletion, archive maintenance, and weight update frequency. Experimental results have shown the effectiveness of the proposed approach. AdaW works well for Pareto fronts with very different shapes: 1) the simplex-like, 2) the inverted simplex-like, 3) the highly nonlinear, 4) the disconnect, 5) the degenerated, 6) the badly-scaled, and 7) the high-dimensional.;https://arxiv.org/abs/1709.02679;;;
Training RNNs as Fast as CNNs;Tao Lei, Yu Zhang;Recurrent neural networks scale poorly due to the intrinsic difficulty in parallelizing their state computations. For instance, the forward pass computation of $h_t$ is blocked until the entire computation of $h_{t-1}$ finishes, which is a major bottleneck for parallel computing. In this work, we propose an alternative RNN implementation by deliberately simplifying the state computation and exposing more parallelism. The proposed recurrent unit operates as fast as a convolutional layer and 5-10x faster than cuDNN-optimized LSTM. We demonstrate the unit's effectiveness across a wide range of applications including classification, question answering, language modeling, translation and speech recognition. We open source our implementation in PyTorch and CNTK.;https://arxiv.org/abs/1709.02755;;;
The Mating Rituals of Deep Neural Networks: Learning Compact Feature Representations through Sexual Evolutionary Synthesis;Audrey Chung, Mohammad Javad Shafiee, Paul Fieguth, Alexander Wong;Evolutionary deep intelligence was recently proposed as a method for achieving highly efficient deep neural network architectures over successive generations. Drawing inspiration from nature, we propose the incorporation of sexual evolutionary synthesis. Rather than the current asexual synthesis of networks, we aim to produce more compact feature representations by synthesizing more diverse and generalizable offspring networks in subsequent generations via the combination of two parent networks. Experimental results were obtained using the MNIST and CIFAR-10 datasets, and showed improved architectural efficiency and comparable testing accuracy relative to the baseline asexual evolutionary neural networks. In particular, the network synthesized via sexual evolutionary synthesis for MNIST had approximately double the architectural efficiency (cluster efficiency of 34.29X and synaptic efficiency of 258.37X) in comparison to the network synthesized via asexual evolutionary synthesis, with both networks achieving a testing accuracy of ~97%.;https://arxiv.org/abs/1709.02043;;;
A Deep Reinforcement Learning Chatbot;Iulian V. Serban, Chinnadhurai Sankar, Mathieu Germain, Saizheng Zhang, Zhouhan Lin, Sandeep Subramanian, Taesup Kim, Michael Pieper, Sarath Chandar, Nan Rosemary Ke, Sai Mudumba, Alexandre de Brebisson, Jose M. R. Sotelo, Dendi Suhubdy, Vincent Michalski, Alexandre Nguyen, Joelle Pineau, Yoshua Bengio;We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than competing systems. Due to its machine learning architecture, the system is likely to improve with additional data.;https://arxiv.org/abs/1709.02349;;;
Phylogenetic Convolutional Neural Networks in Metagenomics;Diego Fioravanti, Ylenia Giarratano, Valerio Maggio, Claudio Agostinelli, Marco Chierici, Giuseppe Jurman, Cesare Furlanello;Background: Convolutional Neural Networks can be effectively used only when data are endowed with an intrinsic concept of neighbourhood in the input space, as is the case of pixels in images. We introduce here Ph-CNN, a novel deep learning architecture for the classification of metagenomics data based on the Convolutional Neural Networks, with the patristic distance defined on the phylogenetic tree being used as the proximity measure. The patristic distance between variables is used together with a sparsified version of MultiDimensional Scaling to embed the phylogenetic tree in a Euclidean space. Results: Ph-CNN is tested with a domain adaptation approach on synthetic data and on a metagenomics collection of gut microbiota of 38 healthy subjects and 222 Inflammatory Bowel Disease patients, divided in 6 subclasses. Classification performance is promising when compared to classical algorithms like Support Vector Machines and Random Forest and a baseline fully connected neural network, e.g. the Multi-Layer Perceptron. Conclusion: Ph-CNN represents a novel deep learning approach for the classification of metagenomics data. Operatively, the algorithm has been implemented as a custom Keras layer taking care of passing to the following convolutional layer not only the data but also the ranked list of neighbourhood of each sample, thus mimicking the case of image data, transparently to the user. Keywords: Metagenomics  Deep learning  Convolutional Neural Networks Phylogenetic trees.;https://arxiv.org/abs/1709.02268; Convolutional Neural Networks; Phylogenetic trees;https://arxiv.org/abs/1709.02268
