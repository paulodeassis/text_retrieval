Resilience of Core-Periphery Networks in the Case of Rich-Club;Matteo Cinelli, Giovanna Ferraro, Antonio Iovanella;Core-periphery networks are structures that present a set of central and densely connected nodes, namely the core, and a set of non-central and sparsely connected nodes, namely the periphery. The rich-club refers to a set in which the highest degree nodes show a high density of connections. Thus, a network that displays a rich-club can be interpreted as a core-periphery network in which the core is made up by a number of hubs. In this paper, we test the resilience of networks showing a progressively denser rich-club and we observe how this structure is able to affect the network measures in terms of both cohesion and efficiency in information flow. Additionally, we consider the case in which, instead of making the core denser, we add links to the periphery. These two procedures of core and periphery thickening delineate a decision process in the placement of new links and allow us to conduct a scenario analysis that can be helpful in the comprehension and supervision of complex networks under the resilience perspective. The advantages of the two procedures, as well as their implications, are discussed in relation to both network effciency and node heterogeneity.;https://arxiv.org/abs/1708.07329
Finding Streams in Knowledge Graphs to Support Fact Checking;Prashant Shiralkar, Alessandro Flammini, Filippo Menczer, Giovanni Luca Ciampaglia;"The volume and velocity of information that gets generated online limits current journalistic practices to fact-check claims at the same rate. Computational approaches for fact checking may be the key to help mitigate the risks of massive misinformation spread. Such approaches can be designed to not only be scalable and effective at assessing veracity of dubious claims, but also to boost a human fact checker's productivity by surfacing relevant facts and patterns to aid their analysis. To this end, we present a novel, unsupervised network-flow based approach to determine the truthfulness of a statement of fact expressed in the form of a (subject, predicate, object) triple. We view a knowledge graph of background information about real-world entities as a flow network, and knowledge as a fluid, abstract commodity. We show that computational fact checking of such a triple then amounts to finding a ""knowledge stream"" that emanates from the subject node and flows toward the object node through paths connecting them. Evaluation on a range of real-world and hand-crafted datasets of facts related to entertainment, business, sports, geography and more reveals that this network-flow model can be very effective in discerning true statements from false ones, outperforming existing algorithms on many test cases. Moreover, the model is expressive in its ability to automatically discover several useful path patterns and surface relevant facts that may help a human fact checker corroborate or refute a claim.";https://arxiv.org/abs/1708.07239
Collaborative Inference of Coexisting Information Diffusions;Yanchao Sun, Cong Qian, Ning Yang, Philip S. Yu;Recently, \textit{diffusion history inference} has become an emerging research topic due to its great benefits for various applications, whose purpose is to reconstruct the missing histories of information diffusion traces according to incomplete observations. The existing methods, however, often focus only on single information diffusion trace, while in a real-world social network, there often coexist multiple information diffusions over the same network. In this paper, we propose a novel approach called Collaborative Inference Model (CIM) for the problem of the inference of coexisting information diffusions. By exploiting the synergism between the coexisting information diffusions, CIM holistically models multiple information diffusions as a sparse 4th-order tensor called Coexisting Diffusions Tensor (CDT) without any prior assumption of diffusion models, and collaboratively infers the histories of the coexisting information diffusions via a low-rank approximation of CDT with a fusion of heterogeneous constraints generated from additional data sources. To improve the efficiency, we further propose an optimal algorithm called Time Window based Parallel Decomposition Algorithm (TWPDA), which can speed up the inference without compromise on the accuracy by utilizing the temporal locality of information diffusions. The extensive experiments conducted on real world datasets and synthetic datasets verify the effectiveness and efficiency of CIM and TWPDA.;https://arxiv.org/abs/1708.06890
GitHub and Stack Over ow: Analyzing Developer Interests Across Multiple Social Collaborative Platforms;Roy Ka-Wei Lee, David Lo;Increasingly, software developers are using a wide array of social collaborative platforms for software development and learning. In this work, we examined the similarities in developer's interests within and across GitHub and Stack Overflow. Our study finds that developers share common interests in GitHub and Stack Overflow, on average, 39% of the GitHub repositories and Stack Over ow questions that a developer had participated fall in the common interests. Also, developers do share similar interests with other developers who co-participated activities in the two platforms. In particular, developers who co-commit and co-pull- request same GitHub repositories and co-answer same Stack Overflow questions, share more common interests compare to other developers who co-participate in other platform activities.;https://arxiv.org/abs/1708.06860
Hierarchical benchmark graphs for testing community detection algorithms;Zhao Yang, Juan I. Perotti, Claudio J. Tessone;Hierarchical organization is an important, prevalent characteristic of complex systems in order to understand their organization, the study of the underlying (generally complex) networks that describe the interactions between their constituents plays a central role. Numerous previous works have shown that many real-world networks in social, biologic and technical systems present hierarchical organization, often in the form of a hierarchy of community structures. Many artificial benchmark graphs have been proposed in order to test different community detection methods, but no benchmark has been developed to throughly test the detection of hierarchical community structures. In this study, we fill this vacancy by extending the Lancichinetti-Fortunato-Radicchi (LFR) ensemble of benchmark graphs, adopting the rule of constructing hierarchical networks proposed by Ravasz and Barab\'asi. We employ this benchmark to test three of the most popular community detection algorithms, and quantify their accuracy using the traditional Mutual Information and the recently introduced Hierarchical Mutual Information. The results indicate that the Ravasz-Barab\'asi-Lancichinetti-Fortunato-Radicchi (RB-LFR) benchmark generates a complex hierarchical structure constituting a challenging benchmark for the considered community detection methods.;https://arxiv.org/abs/1708.06969
Network community detection using modularity density measures;Tianlong Chen, Pramesh Singh, Kevin E. Bassler;Modularity, since its introduction, has remained one of the most widely used metrics to assess the quality of community structure in a complex network. However the resolution limit problem associated with modularity limits its applicability to networks with community sizes smaller than a certain scale. In the past various attempts have been made to solve this problem. More recently a new metric, modularity density, was introduced for the quality of community structure in networks in order to solve some of the known problems with modularity, particularly the resolution limit problem. Modularity density resolves some communities which are otherwise undetectable using modularity. However, we find that it does not solve the resolution limit problem completely by investigating some cases where it fails to detect expected community structures. To address this problem, we introduce a variant of this metric and show that it further reduces the resolution limit problem, effectively eliminating the problem in a wide range of networks.;https://arxiv.org/abs/1708.06810
Exploring the Ideological Nature of Journalists' Social Networks on Twitter and Associations with News Story Content;John Wihbey, Thalita Dias Coleman, Kenneth Joseph, David Lazer;The present work proposes the use of social media as a tool for better understanding the relationship between a journalists' social network and the content they produce. Specifically, we ask: what is the relationship between the ideological leaning of a journalist's social network on Twitter and the news content he or she produces? Using a novel dataset linking over 500,000 news articles produced by 1,000 journalists at 25 different news outlets, we show a modest correlation between the ideologies of who a journalist follows on Twitter and the content he or she produces. This research can provide the basis for greater self-reflection among media members about how they source their stories and how their own practice may be colored by their online networks. For researchers, the findings furnish a novel and important step in better understanding the construction of media stories and the mechanics of how ideology can play a role in shaping public information.;https://arxiv.org/abs/1708.06727
Dynamics of organizational culture: Individual beliefs vs. social conformity;Christos Ellinas, Neil Allan, Anders Johansson;The complex nature of organizational culture challenges our ability to infers its underlying dynamics from observational studies. Recent computational studies have adopted a distinct different view, where plausible mechanisms are proposed to describe a wide range of social phenomena, including the onset and evolution of organizational culture. In this spirit, this work introduces an empirically-grounded, agent-based model which relaxes a set of assumptions that describes past work - (a) omittance of an individual's strive for achieving cognitive coherence, (b) limited integration of important contextual factors - by utilizing networks of beliefs and incorporating social rank into the dynamics. As a result, we illustrate that: (i) an organization may appear to be increasingly coherent in terms of organizational culture, yet be composed of individuals with reduced levels of coherence, (ii) the components of social conformity - peer-pressure and social rank - are influential at different aggregation levels.;https://arxiv.org/abs/1708.06736
Structure constrained by metadata in networks of chess players;Nahuel Almeira, Ana Laura Schaigorodsky, Juan Ignacio Perotti, Orlando Vito Billoni;Chess is an emblematic sport that stands out because of its age, popularity and complexity. It has served to study human behavior from the perspective of a wide number of disciplines, from cognitive skills such as memory and learning, to aspects like innovation and decision making. Given that an extensive documentation of chess games played throughout the history is available, it is possible to perform detailed and statistically significant studies about this sport. Here we use one of the most extensive chess databases in the world to construct two networks of chess players. One of the networks includes games that were played over-the-board and the other is related to games played on the Internet. We studied the main topological characteristics of the networks, such as degree distribution and correlation, transitivity and community structure. We complemented the structural analysis by incorporating players' level of play as node metadata. While the two networks are topologically different, we found that in both cases players gather in communities according to their expertise and that an emergent rich-club structure, composed by the top-rated players, is also present.;https://arxiv.org/abs/1708.06694
Min-plus algebraic low rank matrix approximation: a new method for revealing structure in networks;James Hook;In this paper we introduce min-plus low rank matrix approximation. By using min and plus rather than plus and times as the basic operations in the matrix multiplication min-plus low rank matrix approximation is able to detect characteristically different structures than classical low rank approximation techniques such as Principal Component Analysis (PCA). We also show how min-plus matrix algebra can be interpreted in terms of shortest paths through graphs, and consequently how min-plus low rank matrix approximation is able to find and express the predominant structure of a network.;https://arxiv.org/abs/1708.06552
Three faces of node importance in network epidemiology: Exact results for small graphs;Petter Holme;We investigate three aspects of the importance of nodes with respect to Susceptible-Infectious-Removed (SIR) disease dynamics: influence maximization (the expected outbreak size given a set of seed nodes), the effect of vaccination (how much deleting nodes would reduce the expected outbreak size) and sentinel surveillance (how early an outbreak could be detected with sensors at a set of nodes). We calculate the exact expressions of these quantities, as functions of the SIR parameters, for all connected graphs of three to seven nodes. We obtain the smallest graphs where the optimal node sets are not overlapping. We find that: node separation is more important than centrality for more than one active node, that vaccination and influence maximization are the most different aspects of importance, and that the three aspects are more similar when the infection rate is low.;https://arxiv.org/abs/1708.06456
A Tutorial on Hawkes Processes for Events in Social Media;Marian-Andrei Rizoiu, Young Lee, Swapnil Mishra, Lexing Xie;This chapter provides an accessible introduction for point processes, and especially Hawkes processes, for modeling discrete, inter-dependent events over continuous time. We start by reviewing the definitions and the key concepts in point processes. We then introduce the Hawkes process, its event intensity function, as well as schemes for event simulation and parameter estimation. We also describe a practical example drawn from social media data - we show how to model retweet cascades using a Hawkes self-exciting process. We presents a design of the memory kernel, and results on estimating parameters and predicting popularity. The code and sample event data are available as an online appendix;https://arxiv.org/abs/1708.06401
ConStance: Modeling Annotation Contexts to Improve Stance Classification;Kenneth Joseph, Lisa Friedland, William Hobbs, Oren Tsur, David Lazer;Manual annotations are a prerequisite for many applications of machine learning. However, weaknesses in the annotation process itself are easy to overlook. In particular, scholars often choose what information to give to annotators without examining these decisions empirically. For subjective tasks such as sentiment analysis, sarcasm, and stance detection, such choices can impact results. Here, for the task of political stance detection on Twitter, we show that providing too little context can result in noisy and uncertain annotations, whereas providing too strong a context may cause it to outweigh other signals. To characterize and reduce these biases, we develop ConStance, a general model for reasoning about annotations across information conditions. Given conflicting labels produced by multiple annotators seeing the same instances with different contexts, ConStance simultaneously estimates gold standard labels and also learns a classifier for new instances. We show that the classifier learned by ConStance outperforms a variety of baselines at predicting political stance, while the model's interpretable parameters shed light on the effects of each context.;https://arxiv.org/abs/1708.06309
Network Model Selection for Task-Focused Attributed Network Inference;Ivan Brugere, Chris Kanich, Tanya Y. Berger-Wolf;Networks are models representing relationships between entities. Often these relationships are explicitly given, or we must learn a representation which generalizes and predicts observed behavior in underlying individual data (e.g. attributes or labels). Whether given or inferred, choosing the best representation affects subsequent tasks and questions on the network. This work focuses on model selection to evaluate network representations from data, focusing on fundamental predictive tasks on networks. We present a modular methodology using general, interpretable network models, task neighborhood functions found across domains, and several criteria for robust model selection. We demonstrate our methodology on three online user activity datasets and show that network model selection for the appropriate network task vs. an alternate task increases performance by an order of magnitude in our experiments.;https://arxiv.org/abs/1708.06303
Tamper-Evident Complex Genomic Networks;Komal Batool, Muaz A. Niazi;Networks are important storage data structures now used to store personal information of individuals around the globe. With the advent of personal genome sequencing, networks are going to be used to store personal genomic sequencing of people. In contrast to social media networks, the importance of relationships in this genomic network is extremely significant. Losing connections between individuals thus implies losing relationship information (E.g. father or son etc.). There currently exists a considerably serious problem in the current approach to storing network data. Simply stated, network data is not tamper-evident. In other words, if some links or nodes were changed/removed/added by a malicious attacker, it would be impossible for the administrator to detect such changes. While, in the current age of social media networks, change in node characteristics and links can be bad in terms of relationships, in the case of networks for storing personal genomes, the results could be truly devastating. Here we present a scheme for building tamper-evident networks using a combination of Cryptographic and Ego-based Network analytic methods. Using actual published data-sets, we also demonstrate the utility and validity of the scheme besides demonstrating its working in various possible scenarios of usage. Results from the extensive experiments demonstrate the validity of the proposed approach.;https://arxiv.org/abs/1708.05926
Agent-based computing from multi-agent systems to agent-based Models: a visual survey;Muaz A. Niazi, Amir Hussain;"Agent-Based Computing is a diverse research domain concerned with the building of intelligent software based on the concept of ""agents"". In this paper, we use Scientometric analysis to analyze all sub-domains of agent-based computing. Our data consists of 1,064 journal articles indexed in the ISI web of knowledge published during a twenty year period: 1990-2010. These were retrieved using a topic search with various keywords commonly used in sub-domains of agent-based computing. In our proposed approach, we have employed a combination of two applications for analysis, namely Network Workbench and CiteSpace - wherein Network Workbench allowed for the analysis of complex network aspects of the domain, detailed visualization-based analysis of the bibliographic data was performed using CiteSpace. Our results include the identification of the largest cluster based on keywords, the timeline of publication of index terms, the core journals and key subject categories. We also identify the core authors, top countries of origin of the manuscripts along with core research institutes. Finally, our results have interestingly revealed the strong presence of agent-based computing in a number of non-computing related scientific domains including Life Sciences, Ecological Sciences and Social Sciences.";https://arxiv.org/abs/1708.05872
Real Time Prediction of Drive by Download Attacks on Twitter;Amir Javed, Pete Burnap, Omer Rana;The popularity of Twitter for information discovery, coupled with the automatic shortening of URLs to save space, given the 140 character limit, provides cyber criminals with an opportunity to obfuscate the URL of a malicious Web page within a tweet. Once the URL is obfuscated the cyber criminal can lure a user to click on it with enticing text and images before carrying out a cyber attack using a malicious Web server. This is known as a drive-by- download. In a drive-by-download a user's computer system is infected while interacting with the malicious endpoint, often without them being made aware, the attack has taken place. An attacker can gain control of the system by exploiting unpatched system vulnerabilities and this form of attack currently represents one of the most common methods employed. In this paper, we build a machine learning model using machine activity data and tweet meta data to move beyond post-execution classification of such URLs as malicious, to predict a URL will be malicious with 99.2% F-measure (using 10-fold cross validation) and 83.98% (using an unseen test set) at 1 second into the interaction with the URL. Thus providing a basis from which to kill the connection to the server before an attack has completed and proactively blocking and preventing an attack, rather than reacting and repairing at a later date.;https://arxiv.org/abs/1708.05831
Fake News in Social Networks;Christoph Aymanns, Jakob Foerster, Co-Pierre Georg;We model the spread of news as a social learning game on a network. Agents can either endorse or oppose a claim made in a piece of news, which itself may be either true or false. Agents base their decision on a private signal and their neighbors' past actions. Given these inputs, agents follow strategies derived via multi-agent deep reinforcement learning and receive utility from acting in accordance with the veracity of claims. Our framework yields strategies with agent utility close to a theoretical, Bayes optimal benchmark, while remaining flexible to model re-specification. Optimized strategies allow agents to correctly identify most false claims, when all agents receive unbiased private signals. However, an adversary's attempt to spread fake news by targeting a subset of agents with a biased private signal can be successful. Even more so when the adversary has information about agents' network position or private signal. When agents are aware of the presence of an adversary they re-optimize their strategies in the training stage and the adversary's attack is less effective. Hence, exposing agents to the possibility of fake news can be an effective way to curtail the spread of fake news in social networks. Our results also highlight that information about the users' private beliefs and their social network structure can be extremely valuable to adversaries and should be well protected.;https://arxiv.org/abs/1708.06233
Efficient Online Inference for Infinite Evolutionary Cluster models with Applications to Latent Social Event Discovery;Wei Wei, Kennth Joseph, Kathleen Carley;The Recurrent Chinese Restaurant Process (RCRP) is a powerful statistical method for modeling evolving clusters in large scale social media data. With the RCRP, one can allow both the number of clusters and the cluster parameters in a model to change over time. However, application of the RCRP has largely been limited due to the non-conjugacy between the cluster evolutionary priors and the Multinomial likelihood. This non-conjugacy makes inference di cult and restricts the scalability of models which use the RCRP, leading to the RCRP being applied only in simple problems, such as those that can be approximated by a single Gaussian emission. In this paper, we provide a novel solution for the non-conjugacy issues for the RCRP and an example of how to leverage our solution for one speci c problem - the social event discovery problem. By utilizing Sequential Monte Carlo methods in inference, our approach can be massively paralleled and is highly scalable, to the extent it can work on tens of millions of documents. We are able to generate high quality topical and location distributions of the clusters that can be directly interpreted as real social events, and our experimental results suggest that the approaches proposed achieve much better predictive performance than techniques reported in prior work. We also demonstrate how the techniques we develop can be used in a much more general ways toward similar problems.;https://arxiv.org/abs/1708.06000
Kirchhoff Index As a Measure of Edge Centrality in Weighted Networks: Nearly Linear Time Algorithms;Huan Li, Zhongzhi Zhang;"Most previous work of centralities focuses on metrics of vertex importance and methods for identifying powerful vertices, while related work for edges is much lesser, especially for weighted networks, due to the computational challenge. In this paper, we propose to use the well-known Kirchhoff index as the measure of edge centrality in weighted networks, called ?-Kirchhoff edge centrality. The Kirchhoff index of a network is defined as the sum of effective resistances over all vertex pairs. The centrality of an edge e is reflected in the increase of Kirchhoff index of the network when the edge e is partially deactivated, characterized by a parameter ?. We define two equivalent measures for ?-Kirchhoff edge centrality. Both are global metrics and have a better discriminating power than commonly used measures, based on local or partial structural information of networks, e.g. edge betweenness and spanning edge centrality. Despite the strong advantages of Kirchhoff index as a centrality measure and its wide applications, computing the exact value of Kirchhoff edge centrality for each edge in a graph is computationally demanding. To solve this problem, for each of the ?-Kirchhoff edge centrality metrics, we present an efficient algorithm to compute its ?-approximation for all the m edges in nearly linear time in m. The proposed ?-Kirchhoff edge centrality is the first global metric of edge importance that can be provably approximated in nearly-linear time. Moreover, according to the ?-Kirchhoff edge centrality, we present a ?-Kirchhoff vertex centrality measure, as well as a fast algorithm that can compute ?-approximate Kirchhoff vertex centrality for all the n vertices in nearly linear time in m.";https://arxiv.org/abs/1708.05959
What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016;Alexander Baturo, Niheer Dasandi, Slava J. Mikhaylov;There is surprisingly little known about agenda setting for international development in the United Nations (UN) despite it having a significant influence on the process and outcomes of development efforts. This paper addresses this shortcoming using a novel approach that applies natural language processing techniques to countries' annual statements in the UN General Debate. Every year UN member states deliver statements during the General Debate on their governments' perspective on major issues in world politics. These speeches provide invaluable information on state preferences on a wide range of issues, including international development, but have largely been overlooked in the study of global politics. This paper identifies the main international development topics that states raise in these speeches between 1970 and 2016, and examine the country-specific drivers of international development rhetoric.;https://arxiv.org/abs/1708.05873
University Twitter Engagement: Using Twitter Followers to Rank Universities;Corren G. McCoy, Michael L. Nelson, Michele C. Weigle;We examine and rank a set of 264 U.S. universities extracted from the National Collegiate Athletic Association (NCAA) Division I membership and global lists published in U.S. News, Times Higher Education, Academic Ranking of World Universities, and Money Magazine. Our University Twitter Engagement (UTE) rank is based on the friend and extended follower network of primary and affiliated secondary Twitter accounts referenced on a university's home page. In rank-to-rank comparisons we observed a significant, positive rank correlation ({\tau}=0.6018) between UTE and an aggregate reputation ranking which indicates that UTE could be a viable proxy for ranking atypical institutions normally excluded from traditional lists. In addition, we significantly reduce the cost of data collection needed to rank each institution by using only web-based artifacts and a publicly accessible Twitter application programming interface (API).;https://arxiv.org/abs/1708.05790
Modeling Spread of Preferences in Social Networks for Sampling-based Preference Aggregation;Swapnil Dhamal, Rohith D. Vallam, Y. Narahari;Given a large population, it is an intensive task to gather individual preferences over a set of alternatives and arrive at an aggregate preference that reflects the collective preference of the population. We show that social network underlying the population can be harnessed to accomplish this task effectively, by sampling preferences of a small subset of representative nodes. Using a Facebook dataset that we created, we develop models that capture the spread of preferences among nodes in a typical social network. We next propose an appropriate objective function for the problem of selecting best representative nodes, and observe that the greedy algorithm exhibits excellent performance. For computational purposes, we devise an algorithm Greedy-min, which provides a performance guarantee for a wide class of popular voting rules. We devise a second algorithm Greedy-sum, which we show performs well in practice. We compare the performance of these proposed algorithms against random-polling and popular centrality measures, and provide a detailed analysis of the obtained results. Our analysis also suggests that selecting representatives using social network information is advantageous for aggregating preferences related to personal topics, while random polling with a reasonable sample size is good enough for aggregating preferences related to social topics.;https://arxiv.org/abs/1708.05690